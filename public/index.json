[{"authors":["rob"],"categories":null,"content":"I\u0026rsquo;m a data and policy analyst with a substantive background in electoral behavior and partisan affect.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1e69b33059cc9342e33da675bf1c651a","permalink":"/author/rob-lytle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rob-lytle/","section":"authors","summary":"I\u0026rsquo;m a data and policy analyst with a substantive background in electoral behavior and partisan affect.","tags":null,"title":"Rob Lytle","type":"authors"},{"authors":null,"categories":["political science","public opinion","ANES","R"],"content":" Many Roads to Success! When first learning to code in R students often feel like there is exactly one way to get to a desired result. One way to load data, one way to draw a density plot, one way to run a regression—this is (emphatically) not true! In almost every case, there are multiple perfectly legitimate ways to conduct an analysis and generate the output you seek, which is great news for a new student! When you have a limited knowledge of R, it is easy to think of coding as a collection of very discrete tasks, where you have one set of functions and syntax to make a scatterplot, and a totally different set of functions and syntax to import data or run a regression. In reality, there is much more similarity between these tasks than difference.\nGetting better at R comes from letting yourself experiment with your coding strategy, learning where you can apply techniques from other applications. This may seem confusing right now—that’s fine! This document is intended to get you up to speed with the basics of R and to illustrate how much freedom you have in conducting analyses with RStudio.\nPackages One of the reasons RStudio has so many different overlapping approaches to problems is because of packages. A package is a library of functions that you install and load to extend RStudio’s functionality. A function which is not part of an external package is said to be part of base-R, that is that it comes built in to R by default. Without a doubt, the most important package is tidyverse, a collection of smaller packages that improve on every aspect of base-R, from loading data, cleaning data, visualizing data, etc. tidyverse was first published in 2013, so older RScripts are likely not to make use of the package—which is evidenced by the complexity (and length) of their code.\nWriting a script Project Directory Before you start coding, you need to create a new project directory. A .proj file simply tells RStudio to start looking for files from whichever directory your .proj file is in—it’s the equivalent of using the setwd(...) function at the top of a script, but you only have to do it one time, even if you have multiple scripts that you’re working in. The other benefit, is that you can share the project directory with an instructor or colleague and your code will function perfectly. If you set a working directory, the person to whom you share a script will have to manually set their own working directory, which can be frustrating, inefficient, and error-prone. It’s a good habit to always work in a project environment.\nSee the screenshots below to set up your project.\nCreating a new project\nPlace the project in a new directory\nWe just need a regular project\nGive your project a name, and tell RStudio where to put it\nCongratulations! You now have a new project open, but you need to make a new script so you can actually write some code! Click the new script button in the upper left, then save the script (ctrl-s on Windows or cmd-s on a Mac), givign it a descriptive name. Your script is in the upper-left quadrant of your RStudio window and it is where you will write the bulk (if not all) of your code. If you want code to be saved you must write it in your script, not the console or terminal window below.\n“Click the \"new script\" button in the upper left corner”\nLoad packages Now that we have a script, we can load our packages, this is done with the library() function. Here we are loading tidyverse and haven. haven is a small package that lets us read Stata datasets (.dta). If you don’t have these installed you can do so by installing them through the “Packages” tab in the bottom right of the screen (as in the screenshots below) or by typing install.packages(c(\"tidyverse\", \"haven\")) in the console. You only need to install packages once; but you need to load them every time you start RStudio.\n#Loading packages library(tidyverse) #We will load tidyverse in every single R script we write. It is invaluable library(haven) #Haven contains the read_dta() function; this allows us to read Stata (.dta) files ## Warning: package \u0026#39;haven\u0026#39; was built under R version 4.0.4 library(ggeffects) #ggeffects contains useful functions for calculating and plotting predicted values You might get a warning that one or more of your packages was built under a different version of R. This is probably not an issue and you shouldn’t worry about it!\nLoad your data To load data in R, it needs to be in a dataframe—an object made up of columns (variables) and rows (observations). We do this by using a function to read the dataset from your computer and the \u0026lt;- operator to put the data into the dataframe:\nstates_df \u0026lt;- read_dta(\u0026quot;data/states.dta\u0026quot;) #You can say this as \u0026quot;states_df gets states.dta. i glimpse(states_df) #take a quick peek at your data, make sure it looks how you expect ## Rows: 50 ## Columns: 137 ## $ state \u0026lt;chr\u0026gt; \u0026quot;Alabama\u0026quot;, \u0026quot;Alaska\u0026quot;, \u0026quot;Arizona\u0026quot;, \u0026quot;Arkansas\u0026quot;, \u0026quot;Califor~ ## $ state_abbr \u0026lt;chr\u0026gt; \u0026quot;AL\u0026quot;, \u0026quot;AK\u0026quot;, \u0026quot;AZ\u0026quot;, \u0026quot;AR\u0026quot;, \u0026quot;CA\u0026quot;, \u0026quot;CO\u0026quot;, \u0026quot;CT\u0026quot;, \u0026quot;DE\u0026quot;, \u0026quot;FL\u0026quot;~ ## $ gundeath_rate16 \u0026lt;dbl\u0026gt; 21.5, 23.3, 5.0, 17.8, 7.9, 14.3, 4.6, 11.0, 12.6, 1~ ## $ gun_murder10 \u0026lt;dbl\u0026gt; 2.8, 2.7, 3.6, 3.2, 3.4, 1.3, 2.7, 4.2, 3.8, 3.9, 0.~ ## $ Abort_rank3 \u0026lt;dbl+lbl\u0026gt; 2, 3, 1, 1, 3, 2, 3, 2, 2, 1, 3, 2, 3, 1, 3, 1, ~ ## $ Abortion_rank12 \u0026lt;dbl\u0026gt; 20, 35, 5, 4, 49, 25, 45, 30, 26, 9, 42, 22, 36, 7, ~ ## $ Adv_or_more \u0026lt;dbl\u0026gt; 7.7, 9.0, 9.3, 6.1, 10.7, 12.7, 15.5, 11.4, 9.0, 9.9~ ## $ BA_or_more \u0026lt;dbl\u0026gt; 22.0, 26.6, 25.6, 18.9, 29.9, 35.9, 35.6, 28.7, 25.3~ ## $ Cig_tax12 \u0026lt;dbl\u0026gt; 0.425, 2.000, 2.000, 1.150, 0.870, 0.840, 3.400, 1.6~ ## $ Cig_tax12_3 \u0026lt;dbl+lbl\u0026gt; 1, 3, 3, 2, 2, 2, 3, 2, 2, 1, 3, 1, 3, 2, 2, 1, ~ ## $ Conserv_advantage \u0026lt;dbl\u0026gt; 36.0, 21.3, 19.5, 26.7, 6.3, 15.7, 1.8, 6.3, 17.2, 2~ ## $ Conserv_public \u0026lt;dbl\u0026gt; 44.7, 43.1, 36.0, 45.2, 30.8, 36.9, 27.4, 32.3, 36.8~ ## $ Dem_advantage \u0026lt;dbl\u0026gt; -14.6, -12.2, -3.5, -1.4, 14.9, -2.4, 17.0, 15.9, 4.~ ## $ Govt_worker \u0026lt;dbl\u0026gt; 17.5, 28.0, 15.5, 17.6, 14.9, 15.7, 15.9, 16.1, 14.5~ ## $ Gun_rank3 \u0026lt;dbl+lbl\u0026gt; 2, 3, 3, 3, 1, 1, 1, 2, 3, 2, 1, 3, 1, 3, 2, 3, ~ ## $ Gun_rank11 \u0026lt;dbl\u0026gt; 17, 50, 50, 39, 1, 15, 5, 18, 41, 22, 6, 47, 9, 39, ~ ## $ Gun_scale11 \u0026lt;dbl\u0026gt; 14, 0, 0, 4, 81, 15, 58, 13, 3, 8, 50, 2, 35, 4, 7, ~ ## $ HR_cons_rank11 \u0026lt;dbl\u0026gt; 151.7143, 200.0000, 155.5714, 132.5000, 274.2885, 16~ ## $ HR_conserv11 \u0026lt;dbl\u0026gt; 65.61905, 55.66667, 62.59524, 69.33333, 54.78931, 59~ ## $ HR_lib_rank11 \u0026lt;dbl\u0026gt; 277.5714, 228.0000, 269.8571, 295.2500, 152.4038, 26~ ## $ HR_liberal11 \u0026lt;dbl\u0026gt; 34.38095, 44.33333, 37.40476, 30.66667, 81.02201, 40~ ## $ HS_or_more \u0026lt;dbl\u0026gt; 82.1, 91.4, 84.2, 82.4, 80.6, 89.3, 88.6, 87.4, 85.3~ ## $ Obama2012 \u0026lt;dbl\u0026gt; 38.36, 40.81, 44.45, 36.88, 60.19, 51.45, 58.06, 58.~ ## $ Obama_win12 \u0026lt;dbl+lbl\u0026gt; 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, ~ ## $ Pop2000 \u0026lt;dbl\u0026gt; 4447100, 626932, 5130632, 2673400, 33871648, 4301261~ ## $ Pop2010 \u0026lt;dbl\u0026gt; 4779736, 710231, 6392017, 2915918, 37253956, 5029196~ ## $ Pop2010_hun_thou \u0026lt;dbl\u0026gt; 47.79736, 7.10231, 63.92017, 29.15918, 372.53956, 50~ ## $ Popchng0010 \u0026lt;dbl\u0026gt; 332636, 83299, 1261385, 242518, 3382308, 727935, 168~ ## $ PopchngPct \u0026lt;dbl\u0026gt; 7.5, 13.3, 24.6, 9.1, 10.0, 16.9, 4.9, 14.6, 17.6, 1~ ## $ Pot_policy \u0026lt;dbl+lbl\u0026gt; 2, 3, 3, 0, 4, 4, 4, 3, 1, 0, 3, 0, 1, 0, 1, 0, ~ ## $ ProChoice \u0026lt;dbl\u0026gt; 36, 58, 56, 40, 65, 61, 68, 63, 58, 52, 57, 41, 58, ~ ## $ ProLife \u0026lt;dbl\u0026gt; 54, 37, 39, 55, 28, 34, 26, 31, 36, 43, 35, 55, 33, ~ ## $ Relig_Cath \u0026lt;dbl\u0026gt; 6.6, 14.6, 27.3, 5.9, 31.9, 21.9, 41.5, 25.6, 25.4, ~ ## $ Relig_Prot \u0026lt;dbl\u0026gt; 79.3, 50.0, 43.3, 78.6, 37.8, 46.1, 32.1, 52.0, 51.2~ ## $ Relig_high \u0026lt;dbl\u0026gt; 55.7, 31.3, 36.6, 52.3, 34.5, 33.5, 30.5, 35.2, 37.6~ ## $ Relig_low \u0026lt;dbl\u0026gt; 14.3, 39.5, 33.9, 18.7, 36.6, 39.3, 40.4, 33.7, 30.7~ ## $ Religiosity3 \u0026lt;dbl+lbl\u0026gt; 3, 1, 2, 3, 1, 1, 1, 2, 2, 3, 1, 3, 2, 2, 2, 3, ~ ## $ Romney2012 \u0026lt;dbl\u0026gt; 60.55, 54.80, 53.48, 60.57, 37.09, 46.09, 40.72, 39.~ ## $ Smokers12 \u0026lt;dbl\u0026gt; 25, 24, 21, 26, 15, 20, 18, 21, 21, 21, 16, 18, 21, ~ ## $ StateID \u0026lt;chr\u0026gt; \u0026quot;AL\u0026quot;, \u0026quot;AK\u0026quot;, \u0026quot;AZ\u0026quot;, \u0026quot;AR\u0026quot;, \u0026quot;CA\u0026quot;, \u0026quot;CO\u0026quot;, \u0026quot;CT\u0026quot;, \u0026quot;DE\u0026quot;, \u0026quot;FL\u0026quot;~ ## $ TO_0812 \u0026lt;dbl\u0026gt; -2.9, -9.4, -3.1, -2.9, -6.5, 0.5, -6.3, -3.5, -4.0,~ ## $ Uninsured_pct \u0026lt;dbl\u0026gt; 18.8, 21.8, 20.5, 21.9, 23.2, 17.1, 9.9, 9.6, 22.8, ~ ## $ abort_rate05 \u0026lt;dbl\u0026gt; 11.9, 13.6, 16.0, 8.3, 27.1, 16.1, 23.6, 28.8, 26.8,~ ## $ abort_rate08 \u0026lt;dbl\u0026gt; 12.0, 12.0, 15.2, 8.7, 27.6, 15.7, 24.6, 40.0, 27.2,~ ## $ abortlaw3 \u0026lt;dbl+lbl\u0026gt; 2, 1, 2, 3, 1, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ~ ## $ abortlaw10 \u0026lt;dbl\u0026gt; 8, 5, 6, 9, 4, 4, 4, 5, 7, 9, 2, 9, 6, 10, 6, 8, 7, ~ ## $ alcohol \u0026lt;dbl\u0026gt; 2.01, 3.02, 2.31, 1.83, 2.33, 2.68, 2.34, 3.13, 2.61~ ## $ attend_pct \u0026lt;dbl\u0026gt; 52, 22, 29, 50, 33, 29, 30, 35, 35, 45, 35, 45, 39, ~ ## $ battle04 \u0026lt;dbl+lbl\u0026gt; 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ~ ## $ blkleg \u0026lt;dbl\u0026gt; 25, 2, 1, 11, 5, 4, 7, 5, 14, 21, 1, 0, 16, 7, 2, 4,~ ## $ blkpct04 \u0026lt;dbl\u0026gt; 26.4, 3.6, 3.5, 15.8, 6.8, 4.1, 10.1, 20.4, 15.7, 29~ ## $ blkpct08 \u0026lt;dbl\u0026gt; 26.4, 4.3, 4.2, 15.8, 6.7, 4.3, 10.3, 20.9, 15.9, 30~ ## $ blkpct10 \u0026lt;dbl\u0026gt; 26.8, 4.7, 5.0, 16.1, 7.2, 5.0, 11.3, 22.9, 17.0, 31~ ## $ bush00 \u0026lt;dbl\u0026gt; 56.48376, 58.62096, 51.02114, 51.30720, 41.65137, 50~ ## $ bush04 \u0026lt;dbl\u0026gt; 62.46, 61.07, 54.83, 54.31, 44.36, 51.71, 43.95, 45.~ ## $ carfatal \u0026lt;dbl\u0026gt; 24.9, 17.4, 20.3, 25.6, 12.1, 17.3, 10.1, 15.0, 19.1~ ## $ carfatal07 \u0026lt;dbl\u0026gt; 25.9, 15.2, 17.6, 23.7, 11.7, 12.3, 8.7, 13.6, 18.1,~ ## $ cig_tax \u0026lt;dbl\u0026gt; 0.425, 2.000, 2.000, 0.590, 0.870, 0.840, 2.000, 1.1~ ## $ cig_tax_3 \u0026lt;dbl+lbl\u0026gt; 1, 3, 3, 1, 2, 2, 3, 2, 1, 1, 3, 1, 2, 2, 2, 2, ~ ## $ cigarettes \u0026lt;dbl\u0026gt; 9.41, 6.22, 2.40, 8.51, 3.69, 5.86, 5.47, 17.12, 6.7~ ## $ college \u0026lt;dbl\u0026gt; 21.1, 26.7, 24.3, 19.1, 29.1, 34.7, 34.6, 27.6, 25.1~ ## $ conpct_m \u0026lt;dbl\u0026gt; 40.67797, 36.32479, 33.33333, 38.87755, 28.46612, 32~ ## $ cons_hr06 \u0026lt;dbl\u0026gt; 77.71429, 72.00000, 69.00000, 56.25000, 37.33962, 63~ ## $ cons_hr09 \u0026lt;dbl\u0026gt; 72.00, 75.00, 49.50, 28.50, 35.09, 30.29, 1.60, 56.0~ ## $ cook_index \u0026lt;dbl\u0026gt; -13.2, -13.4, -6.1, -8.8, 7.4, -0.2, 7.1, 7.0, -1.8,~ ## $ cook_index3 \u0026lt;dbl+lbl\u0026gt; 1, 1, 2, 1, 3, 2, 3, 3, 2, 2, 3, 1, 3, 2, 2, 1, ~ ## $ defexpen \u0026lt;dbl\u0026gt; 1757, 3556, 1771, 530, 1106, 1139, 2453, 690, 938, 1~ ## $ demHR11 \u0026lt;dbl\u0026gt; 14.28571, 0.00000, 28.57143, 25.00000, 64.15094, 42.~ ## $ dem_hr09 \u0026lt;dbl\u0026gt; 42.85714, 0.00000, 62.50000, 75.00000, 64.15094, 71.~ ## $ demnat06 \u0026lt;dbl\u0026gt; 22.22222, 0.00000, 20.00000, 83.33333, 63.63636, 44.~ ## $ dempct_m \u0026lt;dbl\u0026gt; 38.87628, 26.14108, 31.93651, 43.06358, 41.29714, 29~ ## $ demstate06 \u0026lt;dbl\u0026gt; 60.71429, 43.33333, 44.44444, 75.55556, 60.83333, 59~ ## $ demstate09 \u0026lt;dbl\u0026gt; 57.85714, 46.66667, 41.11111, 72.59259, 64.16667, 59~ ## $ demstate13 \u0026lt;dbl\u0026gt; 35.71429, 36.66667, 41.11111, 46.66667, 66.66667, 55~ ## $ density \u0026lt;dbl\u0026gt; 94.4, 1.2, 56.3, 56.0, 239.1, 48.5, 738.1, 460.8, 35~ ## $ division \u0026lt;dbl+lbl\u0026gt; 6, 9, 8, 7, 9, 8, 1, 5, 5, 5, 9, 8, 3, 3, 4, 4, ~ ## $ earmarks_pcap \u0026lt;dbl\u0026gt; 38.9, 425.5, 20.9, 26.7, 12.5, 22.5, 14.8, 25.7, 14.~ ## $ evm \u0026lt;dbl\u0026gt; 9, 3, 10, 6, 0, 0, 0, 0, 0, 15, 0, 4, 0, 0, 0, 6, 8,~ ## $ evo \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 55, 9, 7, 3, 27, 0, 4, 0, 21, 11, 7, 0, ~ ## $ evo2012 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 55, 9, 7, 3, 29, 0, 4, 0, 20, 0, 6, 0, 0~ ## $ evr2012 \u0026lt;dbl\u0026gt; 9, 3, 11, 6, 0, 0, 0, 0, 0, 16, 0, 4, 0, 11, 0, 6, 8~ ## $ gay_policy \u0026lt;dbl+lbl\u0026gt; 4, 3, 3, 4, 2, 2, 1, 3, 4, 4, 2, 4, 2, 3, 1, 4, ~ ## $ gay_policy2 \u0026lt;dbl+lbl\u0026gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, ~ ## $ gay_policy_con \u0026lt;dbl+lbl\u0026gt; 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, ~ ## $ gay_support \u0026lt;dbl\u0026gt; 44, 56, 58, 44, 64, 61, 65, 60, 57, 51, 62, 47, 60, ~ ## $ gay_support3 \u0026lt;dbl+lbl\u0026gt; 1, 2, 2, 1, 3, 3, 3, 2, 2, 1, 3, 1, 2, 2, 2, 2, ~ ## $ gb_win00 \u0026lt;dbl+lbl\u0026gt; 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, ~ ## $ gb_win04 \u0026lt;dbl+lbl\u0026gt; 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, ~ ## $ gore00 \u0026lt;dbl\u0026gt; 41.56650, 27.66634, 44.73459, 45.86426, 53.44957, 42~ ## $ gun_check \u0026lt;dbl\u0026gt; 9024.683, 12016.091, 5313.863, 8443.070, 3040.222, 8~ ## $ gun_dealer \u0026lt;dbl\u0026gt; 47.36663, 139.25047, 45.44731, 67.42302, 21.60307, 5~ ## $ gun_rank_rev \u0026lt;dbl\u0026gt; 30, 6, 13, 13, 48, 32, 46, 38, 13, 16, 41, 13, 41, 1~ ## $ gunlaw_rank \u0026lt;dbl\u0026gt; 19, 43, 36, 36, 1, 17, 3, 11, 36, 33, 8, 36, 8, 31, ~ ## $ gunlaw_rank3_rev \u0026lt;dbl+lbl\u0026gt; 2, 1, 1, 1, 3, 3, 3, 3, 1, 2, 3, 1, 3, 2, 3, 2, ~ ## $ gunlaw_scale \u0026lt;dbl\u0026gt; 15, 4, 6, 6, 79, 16, 54, 22, 6, 7, 43, 6, 28, 8, 16,~ ## $ hispanic04 \u0026lt;dbl\u0026gt; 2.2, 4.9, 28.0, 4.4, 34.7, 19.1, 10.6, 5.8, 19.0, 6.~ ## $ hispanic08 \u0026lt;dbl\u0026gt; 2.9, 6.1, 30.1, 5.6, 36.6, 20.2, 12.0, 6.8, 21.0, 8.~ ## $ hispanic10 \u0026lt;dbl\u0026gt; 3.9, 5.5, 29.6, 6.4, 37.6, 20.7, 13.4, 8.2, 22.5, 8.~ ## $ indpct_m \u0026lt;dbl\u0026gt; 29.96595, 43.56846, 29.71429, 35.93449, 25.82884, 37~ ## $ kerry04 \u0026lt;dbl\u0026gt; 36.84, 35.52, 44.37, 44.55, 54.31, 47.04, 54.31, 53.~ ## $ libpct_m \u0026lt;dbl\u0026gt; 16.82809, 17.94872, 19.23584, 16.83673, 24.20596, 21~ ## $ mccain08 \u0026lt;dbl\u0026gt; 60.32, 59.42, 53.39, 58.72, 36.91, 44.71, 38.22, 36.~ ## $ modpct_m \u0026lt;dbl\u0026gt; 42.49395, 45.72650, 47.43083, 44.28571, 47.32791, 45~ ## $ nader00 \u0026lt;dbl\u0026gt; 1.0996404, 10.0668861, 2.9794075, 1.4559857, 3.81827~ ## $ obama08 \u0026lt;dbl\u0026gt; 38.74, 37.89, 44.91, 38.86, 60.94, 53.66, 60.59, 61.~ ## $ obama_win08 \u0026lt;dbl+lbl\u0026gt; 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, ~ ## $ over64 \u0026lt;dbl\u0026gt; 13.2, 6.4, 12.7, 13.8, 10.7, 9.8, 13.5, 13.1, 16.8, ~ ## $ permit \u0026lt;dbl\u0026gt; 27.6, NA, 46.2, 21.1, 52.8, 46.3, 45.3, NA, 37.8, 27~ ## $ pop_18_24 \u0026lt;dbl\u0026gt; 10.004388, 11.144578, 9.614413, 10.075567, 9.952397,~ ## $ pop_18_24_10 \u0026lt;dbl\u0026gt; 10.020921, 10.563380, 9.903004, 9.739369, 10.530413,~ ## $ prcapinc \u0026lt;dbl\u0026gt; 27795, 34454, 28442, 25725, 35019, 36063, 45398, 358~ ## $ region \u0026lt;dbl+lbl\u0026gt; 3, 4, 4, 3, 4, 4, 1, 3, 3, 3, 4, 4, 2, 2, 2, 2, ~ ## $ relig_import \u0026lt;dbl\u0026gt; 58.47953, NA, 33.20000, 53.13433, 28.78619, 26.08696~ ## $ religiosity \u0026lt;dbl\u0026gt; -13, -177, -140, -23, -147, -163, -170, -109, -84, -~ ## $ reppct_m \u0026lt;dbl\u0026gt; 31.15778, 30.29046, 38.34921, 21.00193, 32.87402, 32~ ## $ rtw \u0026lt;dbl+lbl\u0026gt; 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, ~ ## $ secularism \u0026lt;dbl\u0026gt; 13, 177, 140, 23, 147, 163, 170, 109, 84, 39, 101, 7~ ## $ secularism3 \u0026lt;dbl+lbl\u0026gt; 1, 3, 3, 1, 3, 3, 3, 2, 2, 1, 2, 2, 2, 1, 2, 1, ~ ## $ seniority_sen2 \u0026lt;dbl+lbl\u0026gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~ ## $ south \u0026lt;dbl+lbl\u0026gt; 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, ~ ## $ to_0004 \u0026lt;dbl\u0026gt; 4.20, -12.39, -2.32, 3.31, 6.63, 0.82, 4.47, 2.53, 8~ ## $ to_0408 \u0026lt;dbl\u0026gt; 4.6, -0.8, 1.9, -0.2, 2.9, 3.1, 2.2, 2.0, 3.1, 5.2, ~ ## $ trnout00 \u0026lt;dbl\u0026gt; 50.59, 67.19, 44.64, 46.93, 54.61, 57.55, 60.86, 57.~ ## $ trnout04 \u0026lt;dbl\u0026gt; 54.79, 54.80, 42.32, 50.24, 61.24, 58.37, 65.33, 59.~ ## $ unemploy \u0026lt;dbl\u0026gt; 5.8, 7.5, 5.1, 5.9, 6.2, 5.4, 4.9, 3.9, 4.6, 4.7, 3.~ ## $ union04 \u0026lt;dbl\u0026gt; 9.7, 20.1, 6.3, 4.8, 16.5, 8.4, 15.3, 12.4, 6.0, 6.4~ ## $ union07 \u0026lt;dbl\u0026gt; 9.5, 23.8, 8.8, 5.4, 16.7, 8.7, 15.6, 12.0, 5.9, 4.4~ ## $ union10 \u0026lt;dbl\u0026gt; 10.9, 22.3, 6.5, 4.2, 17.2, 7.0, 17.3, 11.9, 5.8, 4.~ ## $ urban \u0026lt;dbl\u0026gt; 55.4, 65.6, 88.2, 52.5, 94.4, 84.5, 87.7, 80.1, 89.3~ ## $ vep00_turnout \u0026lt;dbl\u0026gt; 51.6, 68.1, 45.6, 47.9, 55.7, 57.5, 61.9, 59.0, 55.9~ ## $ vep04_turnout \u0026lt;dbl\u0026gt; 57.2, 69.1, 54.1, 53.6, 58.8, 66.7, 65.0, 64.2, 64.4~ ## $ vep08_turnout \u0026lt;dbl\u0026gt; 61.8, 68.3, 56.0, 53.4, 61.7, 69.8, 67.2, 66.2, 67.5~ ## $ vep12_turnout \u0026lt;dbl\u0026gt; 58.9, 58.9, 52.9, 50.5, 55.2, 70.3, 60.9, 62.7, 63.5~ ## $ womleg_2007 \u0026lt;dbl\u0026gt; 12.9, 21.7, 34.4, 20.7, 28.3, 35.0, 28.3, 30.6, 23.8~ ## $ womleg_2010 \u0026lt;dbl\u0026gt; 12.9, 21.7, 31.1, 23.0, 27.5, 37.0, 31.6, 25.8, 23.1~ ## $ womleg_2011 \u0026lt;dbl\u0026gt; 13.6, 23.3, 34.4, 22.2, 28.3, 41.0, 29.9, 25.8, 25.6~ ## $ abbr_merge \u0026lt;dbl+lbl\u0026gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ~ Above, we used the function read_dta() from the haven package, because our dataset is a .dta file. If we had a .csv or .rds file, we would use the functions read_csv() or read_rds() from `tidyverse.\nYou can name your dataframe anything (besides TRUE and FALSE), but try to make it descriptive to you. Since our dataset is called states.dta, I called the dataframe states_df because it is short and descriptive.\nStart visualizing your data Density plots Let’s make a few density plots of the variable womleg_2011, the proportion of women in the state legislature in the year 2011.\n#Here is how you can make a density plot in base-R. I do not recommend this approach, but it is worthwhile learning it to build your understanding or R. #In your work, you are quite likely to come across older Rscripts which use these functions, and you should know how to read those. plot(density(states_df$womleg_2011)) #qplot is a function in the ggplot package that functions similarly to plot() in base-R #qplot picks the appropriate plot style automatically based on the number and type of variables that you input. #qplot is handy when you want to get a quick peek at the data, but you\u0026#39;re better off using ggplot() if you want to generate a publication quality visual. qplot(states_df$womleg_2011) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Here is a density plot of womleg_2011 in ggplot() this probably looks familiar to you from last semester ggplot(data = states_df, aes(x = womleg_2011)) + geom_density() As you can see, these three approaches (plot(), qplot(), and ggplot()) look quite different, but they all generate plots which look about the same. ggplot() is the approach I recommend for almost every application. It’s very easy to extend, customize, and label your plot in ways that are difficult with plot(), or qplot().\nNow let’s try making a scatterplot with ggplot(). Here we’re using variables for percent of the legislature who are women and the percentage of unionized workers in the laborforce.\nScatterplot ggplot(data = states_df, aes(x = union10, y = womleg_2011)) + geom_point() +#the geom_point() argument tells ggplot to make a scatterplot with these data labs(x = \u0026quot;Percent Workers Unionized (2010)\u0026quot;, #adding some nice labels y = \u0026quot;Percent of Legislature Women (2011)\u0026quot;, title = \u0026quot;Unionization and Women\u0026#39;s Representation\u0026quot;) Regression Taking those same variables, let’s run a bivariate regression analysis. A linear regression is done with the function lm(). The first argument in lm() is the regression formula. Write your dependent (outcome) variable on the left hand side and our independent variable on the right hand side. In place of =, we have to use the ~ in the formula (since = already means something to R).\nbv_model \u0026lt;- lm(womleg_2011 ~ union10, data = states_df) #running the regression and putting it in an object summary(bv_model) ## ## Call: ## lm(formula = womleg_2011 ~ union10, data = states_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.5090 -4.6032 -0.3062 4.0011 19.0342 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 19.0068 1.9566 9.714 6.49e-13 *** ## union10 0.4227 0.1538 2.749 0.00841 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 6.234 on 48 degrees of freedom ## Multiple R-squared: 0.136, Adjusted R-squared: 0.118 ## F-statistic: 7.555 on 1 and 48 DF, p-value: 0.008407 summary(bv_model) returns lots of useful information about our regression. We see the estimate of the coefficient (the slope of our line). In this case, our coefficient estimate tells us that a one unit increase in union membership is associated with a .4227 unit increase in women’s descriptive representation. We also see the p-value, which tells us the probability of observing a relationship as (or more) extreme than this one by chance. In other words, if the null hypothesis is true, we would expect to find an association as strong or stronger than .4227 about \\(.8\\%\\) of the time. This is a statistically significant result.\n#running some predictions. Predicting the percent of women in the legislature at min/max valu predict(bv_model, data.frame(union10 = min(states_df$union10))) #min() and max() are functions that return the min/max of a variable ## 1 ## 20.31722 predict(bv_model, data.frame(union10 = max(states_df$union10))) ## 1 ## 29.65922 #lets plot this line in our ggplot() #create a dataframe containing all values of the regression line bv_predict \u0026lt;- ggemmeans(bv_model, terms = \u0026quot;union10\u0026quot;) #Here you use quotation marks around the column. This is an exception to the rule---i\u0026#39;m not sure why! ## Loading required namespace: emmeans glimpse(bv_predict) ## Rows: 24 ## Columns: 6 ## $ x \u0026lt;int\u0026gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,~ ## $ predicted \u0026lt;dbl\u0026gt; 20.27495, 20.69766, 21.12038, 21.54309, 21.96581, 22.38852, ~ ## $ std.error \u0026lt;dbl\u0026gt; 1.5586346, 1.4344500, 1.3165308, 1.2067151, 1.1074163, 1.021~ ## $ conf.low \u0026lt;dbl\u0026gt; 17.14110, 17.81351, 18.47332, 19.11683, 19.73920, 20.33425, ~ ## $ conf.high \u0026lt;dbl\u0026gt; 23.40879, 23.58182, 23.76744, 23.96936, 24.19242, 24.44280, ~ ## $ group \u0026lt;fct\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ glimpse()ing bv_predict, we see several columns. x is simply all the x values at which we have observations and predicted is the estimated y value of the regression line. std.error is the standard error of the estimate. conf.low and conf.high are the upper and lower confidence intervals. We’ll cover group a bit later.\nNow, we can add this prediction to our ggplot(), by adding our predicted dataframe to geom_line()! In ggplot(), you can call more than one dataframe by adding the data = ... and aes() arguments to your geom_*. You don’t need to do this too often, but is helpful in this case. If you don’t call data = or aes() in your geometry, the geom_*() argument will simply use whatever data and aesthetics you called initially.\nTo add your regression line to the scatterplot, you can copy-and-paste your scatterplot code from above and add geom_line(data = bv_predict, aes(x = x, y = predicted)), like so. Remember to add a + to each function except for the last.\nggplot(data = states_df, aes(x = union10, y = womleg_2011)) + geom_point() +#the geom_point() argument tells ggplot to make a scatterplot with these data geom_line(data = bv_predict, aes(x = x, y = predicted)) + labs(x = \u0026quot;Percent Workers Unionized (2010)\u0026quot;, #adding some nice labels y = \u0026quot;Percent of Legislature Women (2011)\u0026quot;, title = \u0026quot;Unionization and Women\u0026#39;s Representation\u0026quot;) Now let’s do the same thing with a multivariate model. We’ve added a control variable cook_index, which shows states’ partisanship—low values indicate a state is more Republican, high values more Democratic. Using the function ggemmeans() we can estimate the “effect” of union membership on women’s representation at different levels of partisanship. This lets us show the predicted “effect” of x on y at diffent levels of our control variable.\nmv_model \u0026lt;- lm(womleg_2011 ~ union10 + cook_index, data = states_df) #run the regression just as before, adding cook_index to your formula summary(mv_model) ## ## Call: ## lm(formula = womleg_2011 ~ union10 + cook_index, data = states_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.1629 -4.0108 -0.3438 2.3514 15.9817 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 25.35596 2.26298 11.205 7.19e-15 *** ## union10 -0.03492 0.17146 -0.204 0.839506 ## cook_index 0.46614 0.11086 4.205 0.000116 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 5.37 on 47 degrees of freedom ## Multiple R-squared: 0.3722, Adjusted R-squared: 0.3455 ## F-statistic: 13.93 on 2 and 47 DF, p-value: 1.776e-05 mv_predict \u0026lt;- ggemmeans(mv_model, terms = c(\u0026quot;union10\u0026quot;, \u0026quot;cook_index\u0026quot;)) #since we have two terms we have to wrap them in the c() function. We are giving `ggemmeans()` a list, not a single value glimpse(mv_predict) ## Rows: 72 ## Columns: 6 ## $ x \u0026lt;int\u0026gt; 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, ~ ## $ predicted \u0026lt;dbl\u0026gt; 19.92320, 24.09984, 28.27648, 19.88828, 24.06492, 28.24156, ~ ## $ std.error \u0026lt;dbl\u0026gt; 1.3453077, 1.6218310, 2.3289851, 1.2506266, 1.4725254, 2.178~ ## $ conf.low \u0026lt;dbl\u0026gt; 17.21679, 20.83713, 23.59116, 17.37234, 21.10258, 23.85894, ~ ## $ conf.high \u0026lt;dbl\u0026gt; 22.62961, 27.36254, 32.96179, 22.40422, 27.02726, 32.62417, ~ ## $ group \u0026lt;fct\u0026gt; -11.43, -2.47, 6.49, -11.43, -2.47, 6.49, -11.43, -2.47, 6.4~ Notice when we glimpse(mv_predict) we get three different values in group. These are different terciles of cook_index, and are calculated automatically by ggemmeans. We can make use of these in our ggplot(), by mapping the color aesthetic to group.\nggplot(data = states_df, aes(x = union10, y = womleg_2011)) + geom_point() +#the geom_point() argument tells ggplot to make a scatterplot with these data geom_line(data = mv_predict, aes(x = x, y = predicted, color = group)) + labs(x = \u0026quot;Percent Workers Unionized (2010)\u0026quot;, #adding some nice labels y = \u0026quot;Percent of Legislature Women (2011)\u0026quot;, color = \u0026quot;Cook Index\u0026quot;, #titling our legend title = \u0026quot;Unionization and Women\u0026#39;s Representation\u0026quot;) Now, instead of one estimate, we have three—one for each tercile of our partisan cook index control variable. When we control for partisanship, the apparent positive relationship between union membership and women’s representation disappears.\n","date":1616976000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616976000,"objectID":"9c15f0e6522c5ae0c3d9909531d1e957","permalink":"/post/undergrad-workshop/","publishdate":"2021-03-29T00:00:00Z","relpermalink":"/post/undergrad-workshop/","section":"post","summary":"Many Roads to Success! When first learning to code in R students often feel like there is exactly one way to get to a desired result. One way to load data, one way to draw a density plot, one way to run a regression—this is (emphatically) not true!","tags":["R Markdown","plot","visualization","rstats","survey","anes","politics","political science","public opinion","tidyverse","tidy data"],"title":"Getting Started with RStudio","type":"post"},{"authors":[],"categories":["polarization","politics","ideology"],"content":" Few topics are as often abused by pundits and political scientists as ideology—particularly as it relates to partisan polarization. Political scientists are uncharacteristically eager to problematize polarization and insinuate bold action to ameliorate—or at least avoid amplifying—polarization. In this post, I argue two related points. First, that ideological extremity and ideological consistency are not synonymous, nor even necessarily correlated. Second, scholars should reflect on the popularity of the polarization subfield relative to others when proposing and evaluating interventions. It is not necessarily that polarization is a uniquely concerning phenomenon, it may simply be that more people write articles about (and thus have found more concerns regarding) polarization. Scholars of polarization should carefully consider the potential downsides of anti-polarization interventions.\nTwo oft-discussed effects of elite polarization include declining legislative productivity and a concomitant decline in the public’s trust in government. Researchers and pundits often propose reforms intended to advantage moderates (often malapropistically referenced as “non-ideological”) candidates over extreme (“ideological” candidates). Such a characterization fundamentally misrepresents what it means to be “ideological”. Breadth—the number of issue areas encompassed by a belief system—and constraint, the correlation between positions related to area i and area j are the defining characteristics of ideology. The extremity of a belief system is incidental as to whether the system constitutes an ideology.\nA definition of ideology which includes “extremity” as a necessary condition for “ideology” would be over-complicated, and normatively troubling. Practically, such a definition would make generalizable comparisons of ideology nearly impossible, as extremity is a wholly context dependent descriptor. Belief systems’ status as “ideologies” would vary election to election and time to time; while nothing of value would be gained over an extremity-naïve conception of ideology. Defining ideology on the basis of extremity would describe the mainstream of political opinion at any given time not only as the most popular strain of thought, but as the default political position—rhetorically divorced from the unsavory domains of dishonest ideologues to its left and right. Ideas from outside the status-quo are de-legitimized—not through critique of their programmatic content but by a semantic sleight of hand.\nFigure 1 Such is the political world created by sloppy application of words like “ideology” and “moderation”. These words are not antonyms. Rather, “ideology” is a noun and “moderation” an adjective which can sometimes be used to describe an ideology! Pointing this out is not (only) pedantry, but has important implications for the way polarization is (or should be) addressed. Figure 1 represents the distributions of three legislators’ issue preferences along a one-dimensional left-right scale. The mean of distributions \\(A\\), \\(B\\), and \\(C\\) are -1, 0, and 1 respectively. Each distribution describes an ideology: one left, one right, one between. Though centrist, candidate \\(B\\)’s belief system is precisely as constrained as \\(A\\) or \\(C\\)—the standard deviation of each distribution is .1.\nAssume polarization causes gridlock because the distributions of polarized legislators’ preferences do not overlap and suppose a reform is introduced to favor moderate candidates over extreme candidates. \\(A\\) loses their election to a candidate with \\(B\\)’s issue preferences. Assuming \\(C\\) retains veto power, the reform has done little to ameliorate gridlock—there is no more overlap between \\(B\\) and \\(C\\) than between \\(A\\) and \\(C\\). The means may be closer, but they still do not agree on any single issue.\nThis is not to say that reforms cannot reduce gridlock, only to illustrate that a reduction in polarization does not guarantee a reduction in harm resulting from polarization. Scholars should consider not just “mean ideology” but the shape of actors’ preference distributions. In a legislature populated by less ideological voters (who have much more variable ideological preferences) as depicted in Figure 2, a reform that removed \\(A\\) or \\(C\\) would indeed be likely to reduce gridlock, though the legislators’ mean ideology are identical to those in Figure 1 each distribution is much wider. If scholars focus narrowly on mean preferences or other convenient, single-value descriptions of ideology (like self-placements or NOMINATE scores) there is no way to know if our world more closely resembles Figure 1 or Figure 2.\nFigure 2 The volume of polarization-centric work has the potential to induce myopia in social scientists, causing them to overestimate the importance and centrality of polarization as a sinister independent variable from which follows so many political ills. Polarization may be a fundamental problem of American politics, or it may just be the subfield in which the most researchers look for problems. In centering polarization and treating it as a first-order normative concern, researchers may overlook or obfuscate other problems in our politics. Even if advantaging PACs would reduce polarization, it does not follow that such a reform would improve the political climate of the U.S.—to say nothing of the material outcomes produced by our political system.\n","date":1614038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614120848,"objectID":"0ea254994a6da4a1092e1ebb20be84ee","permalink":"/post/please-i-m-begging-you-mdash-stop-trying-to-solve-polarization/","publishdate":"2021-02-23T00:00:00Z","relpermalink":"/post/please-i-m-begging-you-mdash-stop-trying-to-solve-polarization/","section":"post","summary":"Few topics are as often abused by pundits and political scientists as ideology—particularly as it relates to partisan polarization. Political scientists are uncharacteristically eager to problematize polarization and insinuate bold action to ameliorate—or at least avoid amplifying—polarization.","tags":[],"title":"Please, I'm Begging You—Stop Trying to Solve Polarization","type":"post"},{"authors":null,"categories":["political science","public opinion","ANES","R"],"content":"At this point, it\u0026rsquo;s well established that the ANES CDF\u0026rsquo;s codebook is not to be trusted ( I\u0026rsquo;m repeating \u0026ldquo;not to be trusted to include a second link!). Recently, I stumbled across another example of incorrect coding in the cumulative data file, this time in VCF0731 - Do you ever discuss politics with your family or friends?\nThe codebook reports 5 levels:\nDo you ever discuss politics with your family or friends? 1. Yes 5. No 8. DK 9. NA INAP. question not used However, when we load the variable and examine the unique values:\n# pulling anes-cdf from a GitHub repository cdf \u0026lt;- rio::import(\u0026quot;https://github.com/RobLytle/intra-party-affect/raw/master/data/raw/cdf-raw-trim.rds\u0026quot;) unique(cdf$VCF0731) ## [1] NA 5 1 6 7 We see a completely different coding scheme. We are left adrift, wondering \u0026ldquo;What is 6? What is 7?\u0026rdquo; Do 1 and 5 really mean \u0026ldquo;yes\u0026rdquo; and \u0026ldquo;no\u0026rdquo;?\nWe may never know.\nFor a survey that costs several million dollars to conduct, you\u0026rsquo;d think we could expect a double-checked codebook (or at least some kind of version control to easily fix these things as they\u0026rsquo;re identified).\nA version of this post was also published on the gojiberries blog, here.\n","date":1598659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598659200,"objectID":"c3b515879fe95289a06c1bd1f9291c3c","permalink":"/post/anes-problems-too/","publishdate":"2020-08-29T00:00:00Z","relpermalink":"/post/anes-problems-too/","section":"post","summary":"At this point, it\u0026rsquo;s well established that the ANES CDF\u0026rsquo;s codebook is not to be trusted ( I\u0026rsquo;m repeating \u0026ldquo;not to be trusted to include a second link!). Recently, I stumbled across another example of incorrect coding in the cumulative data file, this time in VCF0731 - Do you ever discuss politics with your family or friends?","tags":["R Markdown","plot","visualization","rstats","survey","anes","politics","political science","public opinion","tidyverse","tidy data"],"title":"ANES Coding Problem: VCF0731","type":"post"},{"authors":[],"categories":["Florida Politics","Politics"],"content":" Ron DeSantis speaking in 2017 | photo by Gage Skidmore In a painfully predictable turn of events, COVID-19 cases have begun to skyrocket in Florida, a few short weeks after state and local government began the process of (prematurely) rolling back public health safeguards. This week, Governor Ron DeSantis performed an impressive rhetorical trick, abdicating not just his own responsibility for the death and illness—but that of all the wealthy consumers and business owners agitating for a return to normalcy at the expense of workers\u0026rsquo; lives.\nDesantis says \u0026ldquo;We\u0026rsquo;re not shutting down. We\u0026rsquo;re going to go forward. We\u0026rsquo;re not rolling back. You have to have society function\u0026rdquo; and that \u0026ldquo;The disease burden is not as significant as it was in March and April,\u0026rdquo; it\u0026rsquo;s “Very important to point out. We’re in a much better position than we were in March or April.”. According to DeSantis, to \u0026ldquo;shut down\u0026rdquo; (institute necessary and reasonable public health protections) the state now would do more harm to the people of Florida than the Coronavirus.\nHere, I take DeSantis at his word. I think he is entirely correct that under his administration reinstating necessary physical distance and stay-at-home orders would be terrible for the median Floridian, but this is entirely by design. In the absence of a robust state or federal response, the second round of layoffs and furloughs would be financially ruinous for hundreds of thousands—if not millions—of Floridians, but this is a false dichotomy. DeSantis would have you believe that the paths forward are predetermined: either we can plunge the state and country into depression, or we can grease the gears of the economy with the infected sputum of the poorest and most vulnerable members of our society. DeSantis, like so many leaders, loathes using the power of the state to protect its people and is too craven to tell them as much. While countries like Canada deploy their modest social safety nets to ameliorate the economic costs of prudent health policy, executives and legislators in the U.S. refuse to acknowledge that they too could create such a net.\nDeSantis, like so many elected officials who strip state healthcare funding or tell the elderly it\u0026rsquo;s their patriotic duty to sacrifice themselves for the economy, knows that it\u0026rsquo;s smart politics to collapse the range of choices presented to the public to \u0026ldquo;definitely lose your job\u0026rdquo; or \u0026ldquo;possibly get COVID-19\u0026rdquo;. Republicans recognize that the vulnerable who are hurt by their policies (or lack thereof) were likely not going to vote for them anyway and it\u0026rsquo;s safe for Democrats to hew to a similar line because as long as there are sociopaths like Dan Patrick on the right, even Cuomo-style Democrats will always be the obvious lesser-of-two-evils choice.\nThis week, Desantis took the all too common responsibility-disappearing act to a new level—not only refusing to keep people safe but placing the blame for the consequences of his inaction on those most hurt by it, blaming \u0026ldquo;overwhelmingly Hispanic farmworkers\u0026rdquo; for the rise in cases. Nevermind that the Governor\u0026rsquo;s statement is factually incorrect in the mind palaces of many of our leaders, a callous reopening is the only rational choice path forward. Because reopening is inevitable—the only alternative to economic ruin—business owners and consumers can\u0026rsquo;t be blamed for a spike in deaths and cases, after all, they have no moral agency; they are just along for the ride. Not content to cushion the economy\u0026rsquo;s crash with the bodies of workers, those in power now vilify the workers for the crash in the first place.\nThe American state is as vast as the expectations for what it can offer its people in times of crisis are minuscule. Far from offering carrots to incentivize Floridians to stay home and stay safe, DeSantis is reluctant to even brandish a stick at business owners, saying that enforcement of disease-prevention regulations would be \u0026ldquo;problematic for a whole host of reasons\u0026hellip; I think we\u0026rsquo;ve just got to trust people, give them opportunity to do good things, make good decisions. I think that works better\u0026rdquo;. Meanwhile, Medecins Sans Frontieres has opened a mission in Florida, providing care and testing to those feeding Florida while the governor sneers at them and dogwhistles to his supporters.\n","date":1592784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592844478,"objectID":"b0e272576199fc86d3193e458b370584","permalink":"/post/banality-of-desantis/","publishdate":"2020-06-22T00:00:00Z","relpermalink":"/post/banality-of-desantis/","section":"post","summary":"Ron DeSantis speaking in 2017 | photo by Gage Skidmore In a painfully predictable turn of events, COVID-19 cases have begun to skyrocket in Florida, a few short weeks after state and local government began the process of (prematurely) rolling back public health safeguards.","tags":["politics","Ron DeSantis","Florida Politics","Coronavirus","COVID-19","State Politics","Governor","Florida"],"title":"Vilifying the Victims of an Ineffectual State","type":"post"},{"authors":null,"categories":["political science","public opinion","ANES","R"],"content":"The American National Election Study (ANES) is one of the longest running survey datasets, stretching back to 1948—offering public opinion researchers excellent continuity over a broad base of questions. ANES data is hosted either as individual survey years, or in the \u0026ldquo;cumulative data file\u0026rdquo; (CDF), which coalesces each biennial (or quadrennial) iteration of the survey into a single dataset.\nThe ANES-CDF certainly hews closer to the ideal of tidy data than some canned datasets but is not without its share of eccentricities. I\u0026rsquo;m cataloging some of the (occasionally baffling) coding choices made by the ANES here, in the hopes that it might save some other public opinion researcher\u0026mdash;especialy a hapless grad student\u0026mdash;a few minutes hours of headache down the road.\nThe CDF does not include every question from the individual surveys. This is not so much a mistake on the part of the ANES, but isn\u0026rsquo;t immediately apparent. The cumulative data file does not contain an exhaustive collection of all of the questions asked by the ANES each year. I\u0026rsquo;m currently working on a project involving within-party divisions, and wanted information about primary vote-choice. Thinking I was clever, I opened the cdf codebook and tried (to no avail) to find a question asking for whom respondents voted for in presidential primaries. Broken, defeated, and wondering why the ANES wouldn\u0026rsquo;t ask such an obvious question; I wearily turned to Google where I stumbled across the ANES Continuity Guide. Using the surprisingly well designed tool, I discovered that the question I was hoping to find was asked by the ANES occasionally, but for what I\u0026rsquo;m sure is a very good reason is excluded from the CDF (please version control the CDF).\nIf the variable you want to pull isn\u0026rsquo;t in the CDF, still take the time to look through the continuity guide. The ANES may have what you\u0026rsquo;re looking for if you\u0026rsquo;re willing to stitch together a few different files.\nThe codebook seems to be wrong about NA values. According to the ANES-CDF codebook, NA values are given a numeric code. For example, the codebook tells us that the 7-point party ID variable is coded such that 0 indicates NA and INAP indicates that the question was not asked. Similarly, the codebook says that feeling thermometers are coded such that 98 and 99 indicate Don't Know and NA respectively, while -8 and -9 represent Don't Know and NA for VCF9255 (satisfaction with democracy).\nLet\u0026rsquo;s not rely on the codebook. Using Rcode, I\u0026rsquo;ll pull a few variables to check how they\u0026rsquo;re coded:\n# pulling anes-cdf from a GitHub repository cdf \u0026lt;- rio::import(\u0026quot;https://github.com/RobLytle/intra-party-affect/raw/master/data/raw/cdf-raw-trim.rds\u0026quot;)%\u0026gt;% select(VCF0004, #year VCF0218, #Democrat FT VCF0224, #Republican FT VCF0301, #Party ID VCF9255, #Satisfied/not with Democracy #education 7cat )%\u0026gt;% rename(year = VCF0004, #renaming variables ft_dem = VCF0218, ft_rep = VCF0224, pid_7 = VCF0301, democ_satis = VCF9255, )%\u0026gt;% filter(year \u0026gt;= 2004)%\u0026gt;% # democ_satis isn't asked until 2004, so I'm trimming the data for ease of illustration glimpse() ## Rows: 13,718 ## Columns: 5 ## $ year \u0026lt;dbl\u0026gt; 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2... ## $ ft_dem \u0026lt;dbl\u0026gt; NA, 85, 15, 50, 30, 30, 30, 70, 50, 15, 85, 60, 70, 40,... ## $ ft_rep \u0026lt;dbl\u0026gt; NA, 40, 85, 97, 85, 30, 60, 50, 30, 97, 60, 60, 50, 60,... ## $ pid_7 \u0026lt;dbl\u0026gt; 4, 3, 7, 4, 7, 4, 7, 2, 3, 7, 3, 3, 3, 6, 3, 3, 3, 6, 4... ## $ democ_satis \u0026lt;dbl\u0026gt; 2, 1, 2, NA, 1, 3, 1, 1, NA, 1, 2, 2, 3, 2, 2, 3, NA, 1... Just by tidyverse::glimpse()ing the data, we can tell that there are, in fact, NA values, contrary to what the codebook says, but let\u0026rsquo;s be certain. I\u0026rsquo;ll throw together a quick visualization of the frequency of each variable\u0026rsquo;s values.\n# Making quick ggplots of frequencies democ \u0026lt;- ggplot(cdf, aes(x = democ_satis)) + geom_bar(aes(y = stat(count))) pid \u0026lt;- ggplot(cdf, aes(x = pid_7)) + geom_bar(aes(y = stat(count)))+ scale_x_continuous(breaks = seq(1, 7, by = 1))+ labs(y = \u0026quot; \u0026quot;) ft \u0026lt;- cdf%\u0026gt;% pivot_longer(ft_dem:ft_rep, names_to = \u0026quot;name\u0026quot;, values_to = \u0026quot;ft\u0026quot;)%\u0026gt;% #combining D and R fts into one column filter(ft \u0026gt;= 90)%\u0026gt;% # just filtering high FTs to make visualization easier ggplot(aes(x = ft)) + geom_bar(aes(y = stat(count))) + scale_x_continuous(breaks = seq(90, 100, by = 1)) + labs(y = \u0026quot; \u0026quot;) #putting each visualization into a single object gridExtra::grid.arrange(democ, pid, ft, ncol = 3) Sure enough, none of the values DK/NA/RF values denoted in the codebook actually appear in the CDF. It seems as though they were switched to NA when the dataset was prepared.\nSome responses are truncated. Feeling thermometer questions ask respondents to rate their warmth towards a group (like \u0026ldquo;Democrats\u0026rdquo; or \u0026ldquo;Catholics\u0026rdquo;) on a scale from 0–100. As seen above though, the ANES feeling thermometer variables max out at 97. 98 and 99 are (supposedly) reserved for NA/Don't Know—though that convention is not currently in use. Likely this was done to save space and avoid 3-digit numbers. Size may have been a valid concern in a time before several-terabyte harddrives were commonplace, but serves no purpose now.\nFrom the researcher\u0026rsquo;s perspective, this truncation is troublesome, but not disastrous. Net group affect—a common measure of polarization (e.g., Iyengar, Sood, and Lelkes 2012)—is measured as \\(FT_{in} - FT_{out}\\). Since the ANES coding scheme essentially subtracts 3 from a sizeable chunk of respondents\u0026rsquo; in-party scores (notice how many 97s there are relative to other high-end responses) studies that use net group affect or something similar are likely to underestimate the degree of polarization—albeit by a small degree.\nThese aren\u0026rsquo;t the only instances of truncation or a misleading codebook entry, here is an example of a variable in which the variable\u0026rsquo;s levels varied year to year—not noted in its codebook entry.\nTakeaways The lesson here is to always validate for yourself what is claimed by a dataset\u0026rsquo;s documentation. These problems usually are not deadly, just easily missed. Implementing some kind of version control on the CDF would presumably make it easier to improve the dataset and fix these headaches without breaking existing scripts/analyses.\n","date":1592524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592524800,"objectID":"4ea79032cd4a21c5b1e385bf9f2125ae","permalink":"/post/anes-problems/","publishdate":"2020-06-19T00:00:00Z","relpermalink":"/post/anes-problems/","section":"post","summary":"The American National Election Study (ANES) is one of the longest running survey datasets, stretching back to 1948—offering public opinion researchers excellent continuity over a broad base of questions. ANES data is hosted either as individual survey years, or in the \u0026ldquo;cumulative data file\u0026rdquo; (CDF), which coalesces each biennial (or quadrennial) iteration of the survey into a single dataset.","tags":["R Markdown","plot","visualization","rstats","survey","anes","politics","political science","public opinion","tidyverse","tidy data"],"title":"ANES Headaches and Headscratchers","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"55d97b209aa8b329fab4ff83b7ff4b2a","permalink":"/project/in-party-affect/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/in-party-affect/","section":"project","summary":"More Americans dislike their *own* party than at any point in the last 40 years.","tags":["political science","partisanship","survey research","public opinion","polarization","affect"],"title":"Declining In-Party Affect","type":"project"}]